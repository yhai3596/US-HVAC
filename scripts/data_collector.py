#!/usr/bin/env python3
"""
HVACé¦–å¸­å•†ä¸šåˆ†æå¸ˆ - æ•°æ®æ”¶é›†å¼•æ“
æ”¯æŒFirecrawlå’Œç½‘ç»œæœç´¢åŒæ¨¡å¼æ•°æ®æ”¶é›†
"""

import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import logging
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DataPoint:
    """æ•°æ®ç‚¹ç±»"""
    source: str
    url: str
    content: str
    data_type: str  # product, news, policy, recall, technical
    timestamp: str
    brand: Optional[str] = None
    sensitivity: str = "public"  # public, restricted, confidential
    confidence: float = 1.0
    metadata: Optional[Dict] = None

class HVACDataCollector:
    def __init__(self, config_path: str = "analysis_config.json"):
        self.config = self.load_analysis_config(config_path)
        self.data_source_manager = None
        self.collected_data: List[DataPoint] = []

    def load_analysis_config(self, config_path: str) -> Dict:
        """åŠ è½½åˆ†æé…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.info(f"å·²åŠ è½½åˆ†æé…ç½®: {config_path}")
                return config
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return {}

    def init_data_source_manager(self):
        """åˆå§‹åŒ–æ•°æ®æºç®¡ç†å™¨"""
        try:
            from data_source_manager import DataSourceManager
            self.data_source_manager = DataSourceManager()
            logger.info("æ•°æ®æºç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.error("æ— æ³•å¯¼å…¥æ•°æ®æºç®¡ç†å™¨")

    async def collect_from_firecrawl(self, urls: List[str],
                                   keywords: List[str] = None) -> List[DataPoint]:
        """ä½¿ç”¨Firecrawlæ–¹å¼æ”¶é›†æ•°æ®"""
        logger.info(f"ä½¿ç”¨Firecrawlæ¨¡å¼æ”¶é›† {len(urls)} ä¸ªURLçš„æ•°æ®")

        # è¿™é‡Œæ¨¡æ‹ŸFirecrawlçš„å®é™…è°ƒç”¨
        # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä¼šè°ƒç”¨çœŸå®çš„Firecrawl API
        data_points = []

        for url in urls:
            try:
                # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†è¿‡ç¨‹
                mock_data = {
                    'source': url,
                    'content': f"ä» {url} æ”¶é›†çš„å†…å®¹...",
                    'timestamp': datetime.now().isoformat()
                }

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                if keywords and any(keyword.lower() in url.lower() for keyword in keywords):
                    data_points.append(DataPoint(
                        source=url,
                        url=url,
                        content=mock_data['content'],
                        data_type='news',
                        timestamp=mock_data['timestamp'],
                        brand=self.extract_brand_from_url(url),
                        metadata={'collection_method': 'firecrawl'}
                    ))

            except Exception as e:
                logger.error(f"æ”¶é›† {url} æ—¶å‡ºé”™: {str(e)}")

        return data_points

    async def collect_from_web_search(self, queries: List[str]) -> List[DataPoint]:
        """ä½¿ç”¨ç½‘ç»œæœç´¢æ–¹å¼æ”¶é›†æ•°æ®"""
        logger.info(f"ä½¿ç”¨ç½‘ç»œæœç´¢æ¨¡å¼æ”¶é›† {len(queries)} ä¸ªæŸ¥è¯¢çš„æ•°æ®")

        data_points = []

        for query in queries:
            try:
                # æ¨¡æ‹Ÿæœç´¢è¿‡ç¨‹
                # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œä¼šè°ƒç”¨çœŸå®çš„æœç´¢å¼•æ“API
                mock_results = [
                    {
                        'title': f"æœç´¢ç»“æœ for {query}",
                        'url': f"https://example.com/result1",
                        'snippet': f"å…³äº {query} çš„ä¿¡æ¯..."
                    }
                ]

                for result in mock_results:
                    data_points.append(DataPoint(
                        source="web_search",
                        url=result['url'],
                        content=f"{result['title']}: {result['snippet']}",
                        data_type='news',
                        timestamp=datetime.now().isoformat(),
                        brand=self.extract_brand_from_query(query),
                        metadata={
                            'collection_method': 'web_search',
                            'query': query,
                            'title': result['title']
                        }
                    ))

            except Exception as e:
                logger.error(f"æœç´¢ {query} æ—¶å‡ºé”™: {str(e)}")

        return data_points

    def extract_brand_from_url(self, url: str) -> Optional[str]:
        """ä»URLæå–å“ç‰Œä¿¡æ¯"""
        brand_patterns = {
            'carrier': 'Carrier',
            'trane': 'Trane',
            'bosch': 'BOSCH',
            'lennox': 'Lennox',
            'goodman': 'Goodman',
            'daikin': 'Daikin'
        }

        url_lower = url.lower()
        for pattern, brand in brand_patterns.items():
            if pattern in url_lower:
                return brand
        return None

    def extract_brand_from_query(self, query: str) -> Optional[str]:
        """ä»æŸ¥è¯¢æå–å“ç‰Œä¿¡æ¯"""
        brand_keywords = {
            'Carrier': ['carrier', 'carrier hvac', 'carrier heating'],
            'Trane': ['trane', 'trane hvac', 'trane heating'],
            'BOSCH': ['bosch', 'bosch hvac', 'bosch heating'],
            'Lennox': ['lennox', 'lennox hvac', 'lennox heating'],
            'Goodman': ['goodman', 'goodman hvac', 'goodman heating'],
            'Daikin': ['daikin', 'daikin hvac', 'daikin heating']
        }

        query_lower = query.lower()
        for brand, keywords in brand_keywords.items():
            if any(keyword in query_lower for keyword in keywords):
                return brand
        return None

    async def collect_brand_data(self, brand: str) -> List[DataPoint]:
        """æ”¶é›†ç‰¹å®šå“ç‰Œçš„æ•°æ®"""
        logger.info(f"å¼€å§‹æ”¶é›†å“ç‰Œæ•°æ®: {brand}")

        if not self.data_source_manager:
            self.init_data_source_manager()

        brand_sources = self.data_source_manager.get_brand_sources(brand)
        urls = [source['url'] for source in brand_sources]

        # æ„å»ºæœç´¢æŸ¥è¯¢
        queries = [
            f"{brand} HVAC æ–°å“å‘å¸ƒ 2024",
            f"{brand} äº§å“å¬å› 2024",
            f"{brand} æŠ€æœ¯åˆ›æ–° HVAC",
            f"{brand} å¸‚åœºç­–ç•¥ åŒ—ç¾"
        ]

        # å¹¶è¡Œæ”¶é›†æ•°æ®
        firecrawl_task = self.collect_from_firecrawl(urls)
        search_task = self.collect_from_web_search(queries)

        firecrawl_data, search_data = await asyncio.gather(firecrawl_task, search_task)

        all_data = firecrawl_data + search_data

        # æ ‡è®°BOSCHç‰¹æ®Šåˆ†æ
        if brand == "BOSCH":
            for data_point in all_data:
                data_point.metadata = data_point.metadata or {}
                data_point.metadata['bosch_priority'] = True
                data_point.metadata['deep_analysis'] = True

        return all_data

    async def collect_policy_data(self) -> List[DataPoint]:
        """æ”¶é›†æ”¿ç­–æ³•è§„æ•°æ®"""
        logger.info("å¼€å§‹æ”¶é›†æ”¿ç­–æ³•è§„æ•°æ®")

        policy_queries = [
            "DOE HVAC èƒ½æ•ˆæ ‡å‡† 2024",
            "AHRI HVAC è®¤è¯ æ ‡å‡†",
            "å·çº§ ç©ºè°ƒ æ¿€åŠ±æ”¿ç­– é€€ç¨",
            "EPA ç¯ä¿æ”¿ç­– HVAC",
            "å»ºç­‘èƒ½æ•ˆæ ‡å‡† HVAC"
        ]

        return await self.collect_from_web_search(policy_queries)

    async def collect_recall_data(self) -> List[DataPoint]:
        """æ”¶é›†äº§å“å¬å›æ•°æ®"""
        logger.info("å¼€å§‹æ”¶é›†äº§å“å¬å›æ•°æ®")

        recall_queries = [
            "HVAC äº§å“å¬å› 2024",
            "ç©ºè°ƒ å¬å› å®‰å…¨ é€šå‘Š",
            "Carrier å¬å› 2024",
            "Trane å¬å› 2024",
            "BOSCH å¬å› 2024",
            "Lennox å¬å› 2024",
            "Goodman å¬å› 2024",
            "Daikin å¬å› 2024"
        ]

        return await self.collect_from_web_search(recall_queries)

    async def collect_regional_data(self, region: str) -> List[DataPoint]:
        """æ”¶é›†åŒºåŸŸå¸‚åœºæ•°æ®"""
        logger.info(f"å¼€å§‹æ”¶é›†åŒºåŸŸæ•°æ®: {region}")

        region_queries = [
            f"{region} ç©ºè°ƒ æ¿€åŠ±æ”¿ç­–",
            f"{region} HVAC å¸‚åœº é”€å”®æ•°æ®",
            f"{region} å»ºç­‘èƒ½æ•ˆ æ ‡å‡†"
        ]

        return await self.collect_from_web_search(region_queries)

    async def run_collection(self) -> List[DataPoint]:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
        logger.info("å¼€å§‹HVACå¸‚åœºæ•°æ®æ”¶é›†æµç¨‹")

        all_data = []

        # æ”¶é›†å“ç‰Œæ•°æ®
        for brand in self.config.get('target_brands', []):
            brand_data = await self.collect_brand_data(brand)
            all_data.extend(brand_data)

        # æ”¶é›†æ”¿ç­–æ³•è§„æ•°æ®
        policy_data = await self.collect_policy_data()
        all_data.extend(policy_data)

        # æ”¶é›†å¬å›æ•°æ®
        recall_data = await self.collect_recall_data()
        all_data.extend(recall_data)

        # æ”¶é›†åŒºåŸŸæ•°æ®
        geo_scope = self.config.get('geographic_scope', 'national')
        if geo_scope != 'national':
            regional_data = await self.collect_regional_data(geo_scope)
            all_data.extend(regional_data)

        # æ•°æ®å»é‡å’ŒéªŒè¯
        all_data = self.deduplicate_data(all_data)
        all_data = self.validate_data(all_data)

        self.collected_data = all_data
        logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›† {len(all_data)} ä¸ªæ•°æ®ç‚¹")

        return all_data

    def deduplicate_data(self, data: List[DataPoint]) -> List[DataPoint]:
        """æ•°æ®å»é‡"""
        seen = set()
        unique_data = []

        for item in data:
            # ä½¿ç”¨URLå’Œå†…å®¹å‰100å­—ç¬¦ä½œä¸ºå»é‡é”®
            key = (item.url, item.content[:100])
            if key not in seen:
                seen.add(key)
                unique_data.append(item)

        logger.info(f"å»é‡å®Œæˆï¼ŒåŸå§‹æ•°æ® {len(data)} -> å»é‡å {len(unique_data)}")
        return unique_data

    def validate_data(self, data: List[DataPoint]) -> List[DataPoint]:
        """æ•°æ®éªŒè¯"""
        valid_data = []

        for item in data:
            # åŸºæœ¬éªŒè¯
            if not item.url or not item.content:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®: ç¼ºå°‘URLæˆ–å†…å®¹")
                continue

            # æ•æ„Ÿä¿¡æ¯æ£€æµ‹
            sensitive_keywords = ['unreleased', 'confidential', 'internal', 'æœªå‘å¸ƒ', 'æœºå¯†']
            if any(keyword in item.content.lower() for keyword in sensitive_keywords):
                item.sensitivity = 'restricted'
                logger.info(f"æ£€æµ‹åˆ°æ•æ„Ÿä¿¡æ¯: {item.source}")

            valid_data.append(item)

        logger.info(f"æ•°æ®éªŒè¯å®Œæˆï¼Œæœ‰æ•ˆæ•°æ® {len(valid_data)}")
        return valid_data

    def save_collected_data(self, filepath: str = "collected_data.json"):
        """ä¿å­˜æ”¶é›†çš„æ•°æ®"""
        data_dict = [asdict(item) for item in self.collected_data]

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_dict, f, indent=2, ensure_ascii=False)

        logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®æ”¶é›†æ‘˜è¦"""
        if not self.collected_data:
            return {}

        summary = {
            'total_data_points': len(self.collected_data),
            'by_brand': {},
            'by_data_type': {},
            'by_sensitivity': {},
            'bosch_data_points': 0,
            'time_range': {
                'earliest': None,
                'latest': None
            }
        }

        for item in self.collected_data:
            # æŒ‰å“ç‰Œç»Ÿè®¡
            if item.brand:
                summary['by_brand'][item.brand] = summary['by_brand'].get(item.brand, 0) + 1

            # æŒ‰æ•°æ®ç±»å‹ç»Ÿè®¡
            summary['by_data_type'][item.data_type] = summary['by_data_type'].get(item.data_type, 0) + 1

            # æŒ‰æ•æ„Ÿåº¦ç»Ÿè®¡
            summary['by_sensitivity'][item.sensitivity] = summary['by_sensitivity'].get(item.sensitivity, 0) + 1

            # BOSCHæ•°æ®ç‚¹ç»Ÿè®¡
            if item.brand == 'BOSCH' or (item.metadata and item.metadata.get('bosch_priority')):
                summary['bosch_data_points'] += 1

            # æ—¶é—´èŒƒå›´
            if not summary['time_range']['earliest'] or item.timestamp < summary['time_range']['earliest']:
                summary['time_range']['earliest'] = item.timestamp
            if not summary['time_range']['latest'] or item.timestamp > summary['time_range']['latest']:
                summary['time_range']['latest'] = item.timestamp

        return summary

async def main():
    """ä¸»å‡½æ•° - æ•°æ®æ”¶é›†æµ‹è¯•"""
    print("HVACæ•°æ®æ”¶é›†å¼•æ“")
    print("=" * 60)

    collector = HVACDataCollector()

    if not collector.config:
        print("âŒ æœªæ‰¾åˆ°åˆ†æé…ç½®æ–‡ä»¶")
        return

    # è¿è¡Œæ•°æ®æ”¶é›†
    data = await collector.run_collection()

    # æ˜¾ç¤ºæ‘˜è¦
    summary = collector.get_data_summary()
    print(f"\nğŸ“Š æ•°æ®æ”¶é›†æ‘˜è¦:")
    print(f"   æ€»æ•°æ®ç‚¹: {summary['total_data_points']}")
    print(f"   æŒ‰å“ç‰Œ: {summary['by_brand']}")
    print(f"   æŒ‰ç±»å‹: {summary['by_data_type']}")
    print(f"   BOSCHæ•°æ®ç‚¹: {summary['bosch_data_points']}")

    # ä¿å­˜æ•°æ®
    collector.save_collected_data()

    print(f"\nâœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå·²ä¿å­˜åˆ° collected_data.json")

if __name__ == "__main__":
    asyncio.run(main())
